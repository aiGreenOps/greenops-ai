const { eseguiRichiestaLLM } = require("../utils/eseguiRichiestaLlm");

exports.getAIResponse = async (req, res) => {
    const { messaggio, posizione } = req.body;

    try {
        res.setHeader("Content-Type", "text/plain; charset=utf-8");

        await eseguiRichiestaLLM({
            messaggioUtente: messaggio,
            posizione,
            modello: "mistral",
            res // ğŸ‘‰ stream diretto, lascia scrivere i chunk su res
        });

        // âš ï¸ NON serve res.end() qui â€” lo fa giÃ  la funzione interna

    } catch (err) {
        console.error("âŒ Errore nel servizio AI:", err.message);
        res.status(500).end("Errore durante la generazione AI.");
    }
};
